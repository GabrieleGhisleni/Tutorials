{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlB6u4tMphoX"
      },
      "source": [
        "# NLTK - Natural Language Tool Kit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K7stUFBjphok"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF-OuFv_phoo",
        "outputId": "f3063ae3-23c7-4758-aa27-4748215a6d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Package state_union is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('state_union')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('vader_lexicon')\n",
        "# if a pop up does not show do that on pycharm!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQtofeNiphoq"
      },
      "source": [
        "## Preprocessing the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgPyc0_hphor"
      },
      "source": [
        "### Tokenizing\n",
        "\n",
        "- Word tokenizers\n",
        "- Sentence tokenizers\n",
        "- Corpora: body of the text and language or topic\n",
        "- Lexicon: investor speak \"bull\" = positive while for english speaker \"bull\" = scary animal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OrwRY6TAphot"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "eg = \"Im here working on this NLTK packages. in ten minutes i will have the CSS lesson, what A\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JJiSIiNphov"
      },
      "source": [
        "#### Split by sentences.\n",
        "We can see at the point, capital letter and so on. but if we have as as Mr.Smith? that is not a new sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfNqLYJhphox",
        "outputId": "a7cb9db2-3f4b-4a2a-e93a-c63e392530c2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Im here working on this NLTK packages.',\n",
              " 'in ten minutes i will have the CSS lesson, what A']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "sent_tokenize(eg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdVWDWxOphoy"
      },
      "source": [
        "### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "scrolled": true,
        "id": "pY_NxXyOpho1"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop = set(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cJNcqzdpho5"
      },
      "source": [
        "To be sure that we are not using word inside the sed we must perform a check one by one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYQPucegpho6",
        "outputId": "2036d5fa-72ea-4b9d-e41d-972a16ac00ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['between', 'do', 'then']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "filtered = [word for word in stop]\n",
        "filtered[0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67LGL4PEpho7"
      },
      "source": [
        "### Stemming\n",
        "\n",
        "- take the root stemm of the word! reading read\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21v8nrAnpho9",
        "outputId": "971d388b-e5f9-4cbf-f7f9-b7cb8640fbc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'nltk.stem.porter.PorterStemmer'>\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "ps = PorterStemmer() # Create the Obj PorterStemmer!\n",
        "print(type(ps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dei8SJ_2pho-",
        "outputId": "ebc26f44-5b6b-4611-8ff8-5903a46152f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tri\n",
            "tri\n",
            "tri\n",
            "tryli\n"
          ]
        }
      ],
      "source": [
        "example = [\"try\",\"trying\",\"tryed\",\"tryly\"]\n",
        "for word in example:\n",
        "    print(ps.stem(word))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgtnHbLIpho_"
      },
      "source": [
        "### Part of Speech tagging\n",
        "\n",
        "Create a tuple with the word and what is in the prashe!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "C_3bsqIwphpA"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mLd1cV5xphpC"
      },
      "outputs": [],
      "source": [
        "train = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lNQwTCQVphpD"
      },
      "outputs": [],
      "source": [
        "custom = PunktSentenceTokenizer(train) # we can skip this part\n",
        "tokenized = custom.tokenize(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7zlw8jephpF",
        "outputId": "4e3fd565-f561-4769-ef4d-76a88e9e581d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
            "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
            "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
            "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
            "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n",
            "[('31', 'CD'), (',', ','), ('2006', 'CD'), ('.', '.')]\n",
            "[('White', 'NNP'), ('House', 'NNP'), ('photo', 'NN'), ('by', 'IN'), ('Eric', 'NNP'), ('DraperEvery', 'NNP'), ('time', 'NN'), ('I', 'PRP'), (\"'m\", 'VBP'), ('invited', 'JJ'), ('to', 'TO'), ('this', 'DT'), ('rostrum', 'NN'), (',', ','), ('I', 'PRP'), (\"'m\", 'VBP'), ('humbled', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('privilege', 'NN'), (',', ','), ('and', 'CC'), ('mindful', 'NN'), ('of', 'IN'), ('the', 'DT'), ('history', 'NN'), ('we', 'PRP'), (\"'ve\", 'VBP'), ('seen', 'VBN'), ('together', 'RB'), ('.', '.')]\n",
            "[('We', 'PRP'), ('have', 'VBP'), ('gathered', 'VBN'), ('under', 'IN'), ('this', 'DT'), ('Capitol', 'NNP'), ('dome', 'NN'), ('in', 'IN'), ('moments', 'NNS'), ('of', 'IN'), ('national', 'JJ'), ('mourning', 'NN'), ('and', 'CC'), ('national', 'JJ'), ('achievement', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('have', 'VBP'), ('served', 'VBN'), ('America', 'NNP'), ('through', 'IN'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('most', 'RBS'), ('consequential', 'JJ'), ('periods', 'NNS'), ('of', 'IN'), ('our', 'PRP$'), ('history', 'NN'), ('--', ':'), ('and', 'CC'), ('it', 'PRP'), ('has', 'VBZ'), ('been', 'VBN'), ('my', 'PRP$'), ('honor', 'NN'), ('to', 'TO'), ('serve', 'VB'), ('with', 'IN'), ('you', 'PRP'), ('.', '.')]\n",
            "[('In', 'IN'), ('a', 'DT'), ('system', 'NN'), ('of', 'IN'), ('two', 'CD'), ('parties', 'NNS'), (',', ','), ('two', 'CD'), ('chambers', 'NNS'), (',', ','), ('and', 'CC'), ('two', 'CD'), ('elected', 'JJ'), ('branches', 'NNS'), (',', ','), ('there', 'EX'), ('will', 'MD'), ('always', 'RB'), ('be', 'VB'), ('differences', 'NNS'), ('and', 'CC'), ('debate', 'NN'), ('.', '.')]\n",
            "[('But', 'CC'), ('even', 'RB'), ('tough', 'JJ'), ('debates', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('conducted', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('civil', 'JJ'), ('tone', 'NN'), (',', ','), ('and', 'CC'), ('our', 'PRP$'), ('differences', 'NNS'), ('can', 'MD'), ('not', 'RB'), ('be', 'VB'), ('allowed', 'VBN'), ('to', 'TO'), ('harden', 'VB'), ('into', 'IN'), ('anger', 'NN'), ('.', '.')]\n",
            "[('To', 'TO'), ('confront', 'VB'), ('the', 'DT'), ('great', 'JJ'), ('issues', 'NNS'), ('before', 'IN'), ('us', 'PRP'), (',', ','), ('we', 'PRP'), ('must', 'MD'), ('act', 'VB'), ('in', 'IN'), ('a', 'DT'), ('spirit', 'NN'), ('of', 'IN'), ('goodwill', 'NN'), ('and', 'CC'), ('respect', 'NN'), ('for', 'IN'), ('one', 'CD'), ('another', 'DT'), ('--', ':'), ('and', 'CC'), ('I', 'PRP'), ('will', 'MD'), ('do', 'VB'), ('my', 'PRP$'), ('part', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "for idx, i in enumerate(tokenized):\n",
        "    words = nltk.word_tokenize(i)\n",
        "    tagged = nltk.pos_tag(words)\n",
        "    print(tagged)\n",
        "    if idx > 10: break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1df3d7WEphpG"
      },
      "source": [
        "- CC coordinating conjunction\n",
        "- CD cardinal digit\n",
        "- DT determiner\n",
        "- EX existential there (like: “there is” … think of it like “there exists”)\n",
        "- FW foreign word\n",
        "- IN preposition/subordinating conjunction\n",
        "- JJ adjective ‘big’\n",
        "- JJR adjective, comparative ‘bigger’\n",
        "- JJS adjective, superlative ‘biggest’\n",
        "- LS list marker 1)\n",
        "- MD modal could, will\n",
        "- NN noun, singular ‘desk’\n",
        "- NNS noun plural ‘desks’\n",
        "- NNP proper noun, singular ‘Harrison’\n",
        "- NNPS proper noun, plural ‘Americans’\n",
        "- PDT predeterminer ‘all the kids’\n",
        "- POS possessive ending parent’s\n",
        "- PRP personal pronoun I, he, she\n",
        "- PRP possessive pronoun my, his, hers\n",
        "- RB adverb very, silently,\n",
        "- RBR adverb, comparative better\n",
        "- RBS adverb, superlative best\n",
        "- RP particle give up\n",
        "- TO, to go ‘to’ the store.\n",
        "- UH interjection, errrrrrrrm\n",
        "- VB verb, base form take\n",
        "- VBD verb, past tense, took\n",
        "- VBG verb, gerund/present participle taking\n",
        "- VBN verb, past participle is taken\n",
        "- VBP verb, sing. present, known-3d take\n",
        "- VBZ verb, 3rd person sing. present takes\n",
        "- WDT wh-determiner which\n",
        "- WP wh-pronoun who, what\n",
        "- WP possessive wh-pronoun whose\n",
        "- WRB wh-adverb where, when"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0iSgnvKphpI"
      },
      "source": [
        "### Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cA1aFhbWphpJ",
        "outputId": "3a13816d-873d-42dc-ef71-91b203d8df50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['PRESIDENT', 'GEORGE', 'W.', 'BUSH', \"'S\", 'ADDRESS', 'BEFORE', 'A', 'JOINT', 'SESSION', 'OF', 'THE', 'CONGRESS', 'ON', 'THE', 'STATE', 'OF', 'THE', 'UNION', 'January', '31', ',', '2006', 'THE', 'PRESIDENT', ':', 'Thank', 'you', 'all', '.'] \n",
            "\n",
            " (S\n",
            "  PRESIDENT/NNP\n",
            "  GEORGE/NNP\n",
            "  W./NNP\n",
            "  BUSH/NNP\n",
            "  'S/POS\n",
            "  ADDRESS/NNP\n",
            "  BEFORE/IN\n",
            "  A/NNP\n",
            "  JOINT/NNP\n",
            "  SESSION/NNP\n",
            "  OF/IN\n",
            "  THE/NNP\n",
            "  CONGRESS/NNP\n",
            "  ON/NNP\n",
            "  THE/NNP\n",
            "  STATE/NNP\n",
            "  OF/IN\n",
            "  THE/NNP\n",
            "  UNION/NNP\n",
            "  January/NNP\n",
            "  31/CD\n",
            "  ,/,\n",
            "  2006/CD\n",
            "  THE/NNP\n",
            "  PRESIDENT/NNP\n",
            "  :/:\n",
            "  Thank/NNP\n",
            "  you/PRP\n",
            "  all/DT\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "custom = PunktSentenceTokenizer(train) \n",
        "tokenized = custom.tokenize(sample_text)\n",
        "for i in tokenized:\n",
        "    words = nltk.word_tokenize(i)\n",
        "    tagged = nltk.pos_tag(words)\n",
        "    chunk = r\"\"\"Chunk: {<VB.?>} \"\"\" \n",
        "    chunkParser = nltk.RegexpParser(chunk)\n",
        "    chunked = chunkParser.parse(tagged)\n",
        "    print(words, \"\\n\\n\", chunked)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9ah2WXYphpK"
      },
      "source": [
        "### Lemmatazing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imzRtEoRphpL"
      },
      "source": [
        "it's similar to stemming but in this case it return a real word!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7oUVx9vphpL",
        "outputId": "6000047a-e2fd-433c-ae46-cdb3169d93de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good\n",
            "good\n",
            "good\n",
            "best\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
        "print(lemmatizer.lemmatize(\"goodest\", pos=\"a\"))\n",
        "print(lemmatizer.lemmatize(\"good\", pos=\"a\"))\n",
        "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz2b5Ep7phpM"
      },
      "source": [
        "the main argument of pos = \"n\" which stay for noun. if you are passing something different you must pass it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUeEHwOqphpN"
      },
      "source": [
        "## WordNet\n",
        "- Find synonimis, meaning and so on!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "yQ4G405lphpN"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnYtSlGGphpO"
      },
      "source": [
        "### Synset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oh0UpAdephpP",
        "outputId": "bcc8d378-3a10-4e01-f631-49e9639154e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('love.n.01'), Synset('love.n.02'), Synset('beloved.n.01'), Synset('love.n.04'), Synset('love.n.05'), Synset('sexual_love.n.02'), Synset('love.v.01'), Synset('love.v.02'), Synset('love.v.03'), Synset('sleep_together.v.01')]\n"
          ]
        }
      ],
      "source": [
        "syns = wordnet.synsets(\"love\")\n",
        "print(syns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRy3Fjl4phpQ",
        "outputId": "40bd029f-c128-4413-91ad-3d9af486841f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Lemma('sleep_together.v.01.sleep_together'), Lemma('sleep_together.v.01.roll_in_the_hay'), Lemma('sleep_together.v.01.love'), Lemma('sleep_together.v.01.make_out'), Lemma('sleep_together.v.01.make_love'), Lemma('sleep_together.v.01.sleep_with'), Lemma('sleep_together.v.01.get_laid'), Lemma('sleep_together.v.01.have_sex'), Lemma('sleep_together.v.01.know'), Lemma('sleep_together.v.01.do_it'), Lemma('sleep_together.v.01.be_intimate'), Lemma('sleep_together.v.01.have_intercourse'), Lemma('sleep_together.v.01.have_it_away'), Lemma('sleep_together.v.01.have_it_off'), Lemma('sleep_together.v.01.screw'), Lemma('sleep_together.v.01.fuck'), Lemma('sleep_together.v.01.jazz'), Lemma('sleep_together.v.01.eff'), Lemma('sleep_together.v.01.hump'), Lemma('sleep_together.v.01.lie_with'), Lemma('sleep_together.v.01.bed'), Lemma('sleep_together.v.01.have_a_go_at_it'), Lemma('sleep_together.v.01.bang'), Lemma('sleep_together.v.01.get_it_on'), Lemma('sleep_together.v.01.bonk')]\n"
          ]
        }
      ],
      "source": [
        "print(syns[9].lemmas())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shVzCwoqphpR"
      },
      "source": [
        "### Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g-B5VGVphpR",
        "outputId": "0884a5ca-1577-4511-9753-971cfdfe75c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('love.v.02')\n",
            "[Lemma('love.v.02.love'), Lemma('love.v.02.enjoy')]\n",
            "Synset('love.v.02')\n",
            "get pleasure from\n"
          ]
        }
      ],
      "source": [
        "print(syns[7])\n",
        "print(syns[7].lemmas())\n",
        "print(syns[7])\n",
        "print(syns[7].definition())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW-3krRRphpR"
      },
      "source": [
        "### Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCuYFNPWphpS",
        "outputId": "bd79b61f-c625-4dd1-cd1a-6afaf0d6afa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I love cooking']\n"
          ]
        }
      ],
      "source": [
        "print(syns[7].examples())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "yQyya9_gphpS"
      },
      "outputs": [],
      "source": [
        "sinonimi = []\n",
        "contrari = []\n",
        "for syn in wordnet.synsets(\"love\"):\n",
        "    for l in syn.lemmas():\n",
        "        sinonimi.append(l)\n",
        "        if l.antonyms():\n",
        "            contrari.append(l.antonyms()[0].name())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZVeNX1zphpS",
        "outputId": "156996b3-8c2b-4a7b-ed9d-d8a186656e50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Lemma('love.n.04.erotic_love'),\n",
              " Lemma('love.n.05.love'),\n",
              " Lemma('sexual_love.n.02.sexual_love'),\n",
              " Lemma('sexual_love.n.02.lovemaking'),\n",
              " Lemma('sexual_love.n.02.making_love')]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "sinonimi[10:15]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke_Um0PKphpT"
      },
      "source": [
        "### Find semantic similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ3GX1lXphpT",
        "outputId": "620978f0-3439-411c-8801-e1fa56de31b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Lemma('ship.n.01.ship')]\n",
            "[Lemma('boat.n.01.boat')]\n"
          ]
        }
      ],
      "source": [
        "w1 = wordnet.synset(\"ship.n.01\")\n",
        "w2 = wordnet.synset(\"boat.n.01\")\n",
        "print(w1.lemmas())\n",
        "print(w2.lemmas())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI-8iieSphpT",
        "outputId": "b6db2444-ca7e-449e-b08e-b395c9cd2b73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9090909090909091\n"
          ]
        }
      ],
      "source": [
        "print(w1.wup_similarity(w2)) # nice similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frK4kngqphpU"
      },
      "source": [
        "## Text Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDnMgNLqphpU"
      },
      "source": [
        "In this case we will cover binary situation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "gtjDE5_-phpU"
      },
      "outputs": [],
      "source": [
        "import nltk \n",
        "import random as rd # we will use that to shuffle the dt\n",
        "from nltk.corpus import movie_reviews # fancy list of reviews labelled!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6AojVg5phpV"
      },
      "source": [
        "### Load pre-labelled reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuB8I-N9phpV",
        "outputId": "2fc49485-1648-4519-9bd9-a46fc63cb1a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__name__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__unicode__',\n",
              " '__weakref__',\n",
              " '_unload',\n",
              " 'subdir',\n",
              " 'unicode_repr']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "dir(movie_reviews)[-15:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbD1iNxuphpW",
        "outputId": "7f2c49a3-fe53-4efd-baf2-17619ff93849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neg\n",
            "neg/cv000_29416.txt\n",
            "['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an'] neg\n"
          ]
        }
      ],
      "source": [
        "documents = []\n",
        "i = 0\n",
        "for category in movie_reviews.categories():\n",
        "    if i == 0:\n",
        "        print(category)\n",
        "    for fileid in movie_reviews.fileids(category):\n",
        "        if i == 0:\n",
        "            print(fileid)\n",
        "        documents.append((list(movie_reviews.words(fileid)), category))\n",
        "        if i == 0:\n",
        "            print(documents[0][0][0:20], documents[0][1])\n",
        "        i+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBnGiq4hphpW",
        "outputId": "702b46c8-5517-4eac-f79e-d97456e88864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['to', 'sum', 'the', 'entire', 'film', '\"', '54', '\"', 'up', 'in'] neg\n"
          ]
        }
      ],
      "source": [
        "rd.shuffle(documents)\n",
        "print(documents[0][0][0:10], documents[0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "8TaWL2YephpW"
      },
      "outputs": [],
      "source": [
        "all_words= []\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "common = set(stopwords.words(\"english\"))\n",
        "punctuation = set([\",\",\".\",\":\",\";\",\"(\",\")\",\"!\",\"?\",\"'\",\" \\\" \", \"-\" ])\n",
        "for w in movie_reviews.words():\n",
        "    if w not in punctuation and w not in common:\n",
        "        all_words.append(w.lower())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZWovPpsphpX"
      },
      "source": [
        "### Nltk FrequencyDistribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5BdEBJvphpX",
        "outputId": "43788d03-197d-4d37-a897-0befae6eeb25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('\"', 17612), ('film', 9517), ('one', 5852), ('movie', 5771), ('like', 3690)]\n",
            "39607\n"
          ]
        }
      ],
      "source": [
        "all_words = nltk.FreqDist(all_words)\n",
        "print(all_words.most_common(5))\n",
        "print(len(all_words)) # 40000 different words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYjiQ0h_phpX",
        "outputId": "3b9decbb-f2e8-4d9c-e91b-ce24bd46c79a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "253"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "all_words[\"stupid\"] # is like a dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq1jZLflphpY"
      },
      "source": [
        "#### Limit the important words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "FfNRfRzwphpZ"
      },
      "outputs": [],
      "source": [
        "word_features = list(all_words.keys())[:3000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "DjJdmEmBphpZ"
      },
      "outputs": [],
      "source": [
        "def find_features(document):\n",
        "    words = set(document)\n",
        "    features = {}\n",
        "    for w in word_features:\n",
        "        features[w] = (w in words)\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "BqyFGl2Ephpa"
      },
      "outputs": [],
      "source": [
        "feature_sets = [(find_features(rev),category) for (rev,category) in documents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWoiCd4Jphpb"
      },
      "source": [
        "This feature_sets is a list of tuple having the first element a dictionary having a dictionary with all the 3000 most common words and value a boolean if they are contained in that reviews and as a second element if the reviews was positive or negative!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTK5mqJOphpc"
      },
      "source": [
        "### Fit the NaiveBayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "py9Nr4cpphpd"
      },
      "outputs": [],
      "source": [
        "train_x = feature_sets[:1900]\n",
        "test_x = feature_sets[1900:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "BjQ0Uth5phpd"
      },
      "outputs": [],
      "source": [
        "classifier = nltk.NaiveBayesClassifier.train(train_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl9G4b68phpe",
        "outputId": "6998563f-7e1a-44b0-f9fe-83a86db0c2af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.81\n"
          ]
        }
      ],
      "source": [
        "print(\"accuracy:\", nltk.classify.accuracy(classifier, test_x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf-Aoq4kphpf",
        "outputId": "c281e6dd-40ef-474c-de1b-f0d865cdfcea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "               atrocious = True              neg : pos    =     11.6 : 1.0\n",
            "                   sucks = True              neg : pos    =      9.7 : 1.0\n",
            "                     ugh = True              neg : pos    =      9.6 : 1.0\n",
            "                  annual = True              pos : neg    =      9.1 : 1.0\n",
            "                 frances = True              pos : neg    =      9.1 : 1.0\n",
            "                    yawn = True              neg : pos    =      8.9 : 1.0\n",
            "           unimaginative = True              neg : pos    =      8.3 : 1.0\n",
            "             silverstone = True              neg : pos    =      7.6 : 1.0\n",
            "                 idiotic = True              neg : pos    =      7.2 : 1.0\n",
            "                  regard = True              pos : neg    =      7.0 : 1.0\n",
            "              schumacher = True              neg : pos    =      7.0 : 1.0\n",
            "                  turkey = True              neg : pos    =      6.5 : 1.0\n",
            "                obstacle = True              pos : neg    =      6.4 : 1.0\n",
            "                 cunning = True              pos : neg    =      6.4 : 1.0\n",
            "                  crappy = True              neg : pos    =      6.3 : 1.0\n",
            "                  sexist = True              neg : pos    =      6.3 : 1.0\n",
            "                  shoddy = True              neg : pos    =      6.3 : 1.0\n",
            "                  poorly = True              neg : pos    =      6.0 : 1.0\n",
            "                  justin = True              neg : pos    =      5.8 : 1.0\n",
            "                 singers = True              pos : neg    =      5.7 : 1.0\n"
          ]
        }
      ],
      "source": [
        "classifier.show_most_informative_features(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tITaFObQphpf"
      },
      "source": [
        "## Pickle to save the algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "RLJmt3jRphph"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "save_classifier = open(\"naivebayes.pickle\", \"wb\")\n",
        "pickle.dump(classifier, save_classifier)\n",
        "save_classifier.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbHkh9oLphpi"
      },
      "source": [
        "### Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "QeaPq2kUphpj"
      },
      "outputs": [],
      "source": [
        "classifier_f= open(\"naivebayes.pickle\",\"rb\")\n",
        "classifier = pickle.load(classifier_f)\n",
        "classifier_f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zlHbgTGephpj",
        "outputId": "325c2969-3a37-4684-fff7-b6d9bb646be0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.81"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "nltk.classify.accuracy(classifier, test_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYHInyLlphpk"
      },
      "source": [
        "## Integration with Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "8b6iMNGaphpl"
      },
      "outputs": [],
      "source": [
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB # not binary but shade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjig_kNjphpm"
      },
      "source": [
        "### Multinomial Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfXsu4dKphpm",
        "outputId": "6e365614-629b-4fbb-9b81-648379d62891"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.84\n"
          ]
        }
      ],
      "source": [
        "Multinom_classifier = SklearnClassifier(MultinomialNB())\n",
        "Multinom_classifier.train(train_x)\n",
        "print(\"accuracy\", nltk.classify.accuracy(Multinom_classifier, test_x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z47OACTyphpn"
      },
      "source": [
        "### Bernoulli Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WcQO-0ophpo",
        "outputId": "b6e30334-f7e0-4917-a5db-fadeb65fee28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.8\n"
          ]
        }
      ],
      "source": [
        "Bernoulli_classifier = SklearnClassifier(BernoulliNB())\n",
        "Bernoulli_classifier.train(train_x)\n",
        "print(\"accuracy\", nltk.classify.accuracy(Bernoulli_classifier, test_x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfyYMnSXphpp"
      },
      "source": [
        "### Whatever models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "gtfvtXYxphpq"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import SVC, LinearSVC, NuSVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33ZprwX6phpr",
        "outputId": "4fd24ba1-8b22-4d89-8478-b279a2deb903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.88\n"
          ]
        }
      ],
      "source": [
        "SVC_class = SklearnClassifier(SVC())\n",
        "SVC_class.train(train_x)\n",
        "print(\"accuracy\", nltk.classify.accuracy(SVC_class, test_x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUBkq7Mpphps"
      },
      "source": [
        "### Combining models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "uz1e6yvlphps"
      },
      "outputs": [],
      "source": [
        "from nltk.classify import ClassifierI\n",
        "from statistics import mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lS8ehwjphpt",
        "outputId": "6f93b64c-f349-479e-bad0-c662782d334c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SklearnClassifier(NuSVC())>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "Multinom_classifier = SklearnClassifier(MultinomialNB())\n",
        "Multinom_classifier.train(train_x)\n",
        "\n",
        "Bernoulli_classifier = SklearnClassifier(BernoulliNB())\n",
        "Bernoulli_classifier.train(train_x)\n",
        "\n",
        "SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
        "SGDClassifier_classifier.train(train_x)\n",
        "\n",
        "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
        "LinearSVC_classifier.train(train_x)\n",
        "\n",
        "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
        "NuSVC_classifier.train(train_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "2eEvHRKvphpu"
      },
      "outputs": [],
      "source": [
        "class VoteClassifier(ClassifierI):\n",
        "    def __init__(self, *classifiers):\n",
        "        self.classifiers = classifiers\n",
        "    \n",
        "    def classify(self,features):\n",
        "        votes = []\n",
        "        for c in self.classifiers:\n",
        "            v = c.classify(features)\n",
        "            votes.append(v)\n",
        "        return mode(votes)\n",
        "    \n",
        "    def confidence(self, features):\n",
        "        votes = []\n",
        "        for c in self.classifiers:\n",
        "            v = c.classify(features)\n",
        "            votes.append(v)\n",
        "        choice_vote =  votes.count(mode(votes))\n",
        "        return (choice_vote/len(votes))\n",
        "    \n",
        "voted_classifier = VoteClassifier(Multinom_classifier, \n",
        "                                  Bernoulli_classifier, \n",
        "                                  SGDClassifier_classifier, \n",
        "                                  LinearSVC_classifier, \n",
        "                                  NuSVC_classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiiuEvZMphpv",
        "outputId": "aed6d3f7-e145-45a5-8362-9c9628d3bcc7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<SklearnClassifier(MultinomialNB())>,\n",
              " <SklearnClassifier(BernoulliNB())>,\n",
              " <SklearnClassifier(SGDClassifier())>,\n",
              " <SklearnClassifier(LinearSVC())>,\n",
              " <SklearnClassifier(NuSVC())>)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "voted_classifier.classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6izNL8wphpv",
        "outputId": "28cdce9c-96d9-4715-af82-61ba8f32bc83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of bouch of 0.88\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy of bouch of\", nltk.classify.accuracy(voted_classifier, test_x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjGmOblAphpw",
        "outputId": "7fbf39b7-7f7c-4b7a-bf8d-6cd24862f48a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neg 0.8\n"
          ]
        }
      ],
      "source": [
        "new_obs = {\"suks\":True, \"works\":False, \"good\":True, \"uni\":True,\"palace\":True}\n",
        "print(voted_classifier.classify(new_obs), voted_classifier.confidence(new_obs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qngzpE_bphpx"
      },
      "source": [
        "to understand which word are relevant to our analysis we have to see what are inside the dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbK0rQqKphpy",
        "outputId": "1ec77f60-3081-4669-ecbf-e9f09f3ea2f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Informative Features\n",
            "               atrocious = True              neg : pos    =     11.6 : 1.0\n",
            "                   sucks = True              neg : pos    =      9.7 : 1.0\n",
            "                     ugh = True              neg : pos    =      9.6 : 1.0\n",
            "                  annual = True              pos : neg    =      9.1 : 1.0\n",
            "                 frances = True              pos : neg    =      9.1 : 1.0\n",
            "                    yawn = True              neg : pos    =      8.9 : 1.0\n",
            "           unimaginative = True              neg : pos    =      8.3 : 1.0\n",
            "             silverstone = True              neg : pos    =      7.6 : 1.0\n",
            "                 idiotic = True              neg : pos    =      7.2 : 1.0\n",
            "                  regard = True              pos : neg    =      7.0 : 1.0\n",
            "              schumacher = True              neg : pos    =      7.0 : 1.0\n",
            "                  turkey = True              neg : pos    =      6.5 : 1.0\n",
            "                obstacle = True              pos : neg    =      6.4 : 1.0\n",
            "                 cunning = True              pos : neg    =      6.4 : 1.0\n",
            "                  crappy = True              neg : pos    =      6.3 : 1.0\n"
          ]
        }
      ],
      "source": [
        "classifier.show_most_informative_features(15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij3oca1Mphpz"
      },
      "source": [
        "## Short sentece dataset - Recap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "GWuuzb72php0"
      },
      "outputs": [],
      "source": [
        "\"https://pythonprogramming.net/static/downloads/short_reviews/\"\n",
        "import requests\n",
        "negative = requests.get(\"https://pythonprogramming.net/static/downloads/short_reviews/negative.txt\").text\n",
        "positive = requests.get(\"https://pythonprogramming.net/static/downloads/short_reviews/positive.txt\").text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHxD2C-kphp0"
      },
      "source": [
        "### Create whole documents with associated label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "vy1IjtOXphp1"
      },
      "outputs": [],
      "source": [
        "documents = []\n",
        "for row in negative.split(\"\\n\"):\n",
        "    documents.append((row, \"Negative\"))\n",
        "for row in positive.split(\"\\n\"):\n",
        "    documents.append((row, \"Positive\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY-ssFpOphp2",
        "outputId": "7f49603b-1c9e-4d12-bdf8-f2db285555d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('predictable and cloying , though brown sugar is so earnest in its yearning for the days before rap went nihilistic that it summons more spirit and bite than your average formulaic romantic quadrangle . ',\n",
              " 'Negative')"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "\n",
        "rd.shuffle(documents)\n",
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf2ijsflphp2"
      },
      "source": [
        "### Create a Frequency distribution of the words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "uEPps2wlphp3"
      },
      "outputs": [],
      "source": [
        "all_words = []\n",
        "negative_word = nltk.tokenize.word_tokenize(negative)\n",
        "positive_word = nltk.tokenize.word_tokenize(positive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "sutg7Htiphp3"
      },
      "outputs": [],
      "source": [
        "\n",
        "common = set(stopwords.words(\"english\"))\n",
        "punctuation = set([\",\",\".\",\":\",\";\",\"(\",\")\",\"!\",\"?\",\"'\",\" \\\" \", \"-\" ])\n",
        "for w in negative_word:\n",
        "    if w not in punctuation and w not in common:\n",
        "        all_words.append(w.lower())\n",
        "for w in positive_word:\n",
        "    if w not in punctuation and w not in common:\n",
        "        all_words.append(w.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tywYRo7jphp4",
        "outputId": "b2bb9b31-fb40-408a-ab9b-cc013f760d11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(\"'s\", 3537), ('film', 1590), ('movie', 1336), (\"n't\", 940), ('one', 739)]\n",
            "20162\n"
          ]
        }
      ],
      "source": [
        "all_words = nltk.FreqDist(all_words)\n",
        "print(all_words.most_common(5))\n",
        "print(len(all_words))\n",
        "\n",
        "word_features = list(all_words.keys())[:6000] # we take the first 6000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tANRH-I4php5"
      },
      "source": [
        "### Create a feature set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "hZc0qKzMphp5"
      },
      "outputs": [],
      "source": [
        "def find_features(document):\n",
        "    words = nltk.word_tokenize(document)\n",
        "    features = {}\n",
        "    for w in word_features:\n",
        "        features[w] = (w in words)\n",
        "    return features\n",
        "\n",
        "feature_set = [(find_features(rev), category) for (rev, category) in documents ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "LzaUom67php7"
      },
      "outputs": [],
      "source": [
        "train_x = feature_set[:10000]\n",
        "test_x = feature_set[10000:]\n",
        "#18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gegmi5fgphp7"
      },
      "outputs": [],
      "source": [
        "# it took around 30m to fit all the models.\n",
        "train = False\n",
        "if train:\n",
        "  Multinom_classifier = SklearnClassifier(MultinomialNB())\n",
        "  Multinom_classifier.train(train_x)\n",
        "  Bernoulli_classifier = SklearnClassifier(BernoulliNB())\n",
        "  Bernoulli_classifier.train(train_x)\n",
        "  SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
        "  SGDClassifier_classifier.train(train_x)\n",
        "  LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
        "  LinearSVC_classifier.train(train_x)\n",
        "  NuSVC_classifier = SklearnClassifier(NuSVC())\n",
        "  NuSVC_classifier.train(train_x)\n",
        "  class VoteClassifier(ClassifierI):\n",
        "      def __init__(self, *classifiers):\n",
        "          self.classifiers = classifiers\n",
        "      def classify(self,features):\n",
        "          votes = []\n",
        "          for c in self.classifiers:\n",
        "              v = c.classify(features)\n",
        "              votes.append(v)\n",
        "          return mode(votes)\n",
        "      def confidence(self, features):\n",
        "          votes = []\n",
        "          for c in self.classifiers:\n",
        "              v = c.classify(features)\n",
        "              votes.append(v)\n",
        "          choice_vote =  votes.count(mode(votes))\n",
        "          return (choice_vote/len(votes))\n",
        "      \n",
        "  voted_classifier = VoteClassifier(Multinom_classifier, \n",
        "                                    Bernoulli_classifier, \n",
        "                                    SGDClassifier_classifier, \n",
        "                                    LinearSVC_classifier, \n",
        "                                    NuSVC_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgvkNyg-php8"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snweaL7Hphp9"
      },
      "source": [
        "## Already implemented a trained algortimhs\n",
        "\n",
        "it will be slower as f. but at least it consider multiple words! \n",
        "Nope does not work, the accuracy is under 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLyzgsT9php-",
        "outputId": "cc120922-8045-475e-d899-5f75fd414159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.843, 'pos': 0.157, 'compound': 0.0772}\n",
            "{'neg': 0.457, 'neu': 0.543, 'pos': 0.0, 'compound': -0.5216}\n",
            "{'neg': 0.31, 'neu': 0.69, 'pos': 0.0, 'compound': -0.5574}\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "print(sid.polarity_scores(\"i just want something that works with italian words\"))\n",
        "print(sid.polarity_scores(\"i just do not love you\"))\n",
        "x = sid.polarity_scores(\" get my weed my from california that’s that shit\")\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5EMAw47php-",
        "outputId": "dd199136-d31f-452d-bdc6-e105012b2333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.69 neu\n"
          ]
        }
      ],
      "source": [
        "score,value = 0,0\n",
        "for tupla in list(x.items()):\n",
        "    if tupla[1] > score:\n",
        "        score = tupla[1]\n",
        "        value = tupla[0]\n",
        "print(score,value)      "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "357.358px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "NLTK Text Analytics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}