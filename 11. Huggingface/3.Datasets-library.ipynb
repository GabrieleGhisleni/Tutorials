{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to load custom dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# argument to pass to pandas.read_csv()\n",
    "# data_files could also be a url\n",
    "\n",
    "# csv\n",
    "local_csv = load_dataset(\"csv\", data_files=\"path-to-file.csv\", sep=\",\")\n",
    "\n",
    "# json single obj\n",
    "local_csv = load_dataset(\"json\", data_files=\"path-to-file.csv\", field=\"data\")\n",
    "\n",
    "# json multiple files\n",
    "data_files = {\"train\": f\"{url}train.json\", \"test\": f\"{url}test-json\"}\n",
    "local_csv = load_dataset(\"json\", data_files=data_files, field=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "dataset = squad.train_test_split(test_size = 0.1)\n",
    "\n",
    "# select and shuffle\n",
    "indices = [0,10,20,40, 15]\n",
    "squad.shuffle().select(indices)\n",
    "\n",
    "# filter the dataframe\n",
    "squad_filtered = squad.filter(lambda x:x[\"title\"].startswith(\"L\"))\n",
    "\n",
    "# flatten\n",
    "squad.flatten()\n",
    "# we have that answers is nested into text and answer_start, with flatten we bring them out ['answers.text', 'answers.answer_start'].\n",
    "\n",
    "# map \n",
    "def lower_case(ex):\n",
    "    return {\"title\": ex[\"title\"].lower()}\n",
    "\n",
    "squad_lower = squad.map(lower_case, batched=True)\n",
    "## Using Dataset.map() with batched=True will be essential to unlock the speed of the “fast” tokenizers \n",
    "\n",
    "# Renaming and filtering\n",
    "drug_dataset = drug_dataset.filter(lambda x: x[\"condition\"] is not None)\n",
    "drug_dataset = drug_dataset.rename_column(original_column_name=\"Unnamed: 0\", new_column_name=\"patient_id\")\n",
    "drug_dataset = drug_dataset.map(lowercase_condition)\n",
    "\n",
    "# Create new columns\n",
    "def compute_review_length(example):\n",
    "    return {\"review_length\": len(example[\"review\"].split())}\n",
    "\n",
    "drug_dataset = drug_dataset.map(compute_review_length)\n",
    "\n",
    "# sort values\n",
    "drug_dataset[\"train\"].sort(\"review_length\")[:3]\n",
    "\n",
    "# removing emoji\n",
    "drug_dataset = drug_dataset.map(lambda x: {\"review\": html.unescape(x[\"review\"])})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into pandas dataframe!\n",
    "dataset.set_format(\"pandas\")\n",
    "# easier way\n",
    "dataset.to_pandas()\n",
    "# back to original\n",
    "dataset.reset_format()\n",
    "# or\n",
    "from datasets import Dataset\n",
    "freq_dataset = Dataset.from_pandas(frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train-test-validation\n",
    "\n",
    "drug_dataset_clean = drug_dataset[\"train\"].train_test_split(train_size=0.8, seed=42)\n",
    "# Rename the default \"test\" split to \"validation\"\n",
    "drug_dataset_clean[\"validation\"] = drug_dataset_clean.pop(\"test\")\n",
    "# Add the \"test\" set to our `DatasetDict`\n",
    "drug_dataset_clean[\"test\"] = drug_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load datasets\n",
    "\n",
    "# arrow format\n",
    "drug_dataset_clean.save_to_disk(\"path\") # save\n",
    "drug_arrow_load = load_from_disk(\"path\") # load\n",
    "\n",
    "# csv\n",
    "for split, dataset in raw_dataset.items():\n",
    "    dataset.to_csv(f\"myDataset-{split}.csv\", index=None) #save\n",
    "\n",
    "data_files  = {\"train\": \"myDataset-train.csv\", \"test\": \"myDataset-test.csv\" }\n",
    "load_dataset(\"csv\", data_files=data_files)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
