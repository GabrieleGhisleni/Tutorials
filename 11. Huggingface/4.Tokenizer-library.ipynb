{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer-library\n",
    "\n",
    "We have to create a new tokenizer if:\n",
    "\n",
    "- New language\n",
    "- New characters\n",
    "- New domain\n",
    "- New style\n",
    "\n",
    "Training a tokenizer is not the same as training a model! Model training uses stochastic gradient descent to make the loss a little bit smaller for each batch. It’s randomized by nature (meaning you have to set some seeds to get the same results when doing the same training twice). Training a tokenizer is a statistical process that tries to identify which subwords are the best to pick for a given corpus, and the exact rules used to pick them depend on the tokenization algorithm. It’s deterministic, meaning you always get the same results when training with the same algorithm on the same corpus.\n",
    "\n",
    "In order to train a new tokenizer we have to first of all collect a corpus of text then choose an architecture and train in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")\n",
    "\n",
    "# Using a Python generator, we can avoid Python loading anything into memory until it’s actually necessary. \n",
    "# To create such a generator, you just to need to replace the brackets with parentheses:\n",
    "\n",
    "def get_training_corpus():\n",
    "    return (raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"] for i in range(0, len(raw_datasets[\"train\"]), 1000))\n",
    "\n",
    "# or better\n",
    "def get_training_corpus():\n",
    "    dataset = raw_datasets[\"train\"]\n",
    "    for start_idx in range(0, len(dataset), 1000):\n",
    "        samples = dataset[start_idx : start_idx + 1000]\n",
    "        yield samples[\"whole_func_string\"]\n",
    "\n",
    "training_corpus = get_training_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we are going to train a new tokenizer, it’s a good idea to do this to avoid starting entirely from scratch. This way, we won’t have to specify anything about the tokenization algorithm or the special tokens we want to use; our new tokenizer will be exactly the same as GPT-2, and the only thing that will change is the vocabulary, which will be determined by the training on our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)\n",
    "\n",
    "# 52000 is the corpus lenght!\n",
    "# Note that AutoTokenizer.train_new_from_iterator() only works if the tokenizer you are using is a “fast” tokenizer.\n",
    "\n",
    "tokenizer.save_pretrained(\"code-search-net-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we’ll first take a look at the preprocessing that each tokenizer applies to text. Here’s a high-level overview of the steps in the tokenization pipeline:\n",
    "\n",
    "<img src=\"./images/tokenizer_2.PNG\" width=\"70%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The **normalization** step involves some general cleanup, such as removing needless whitespace, \n",
    "# lowercasing, and/or removing accents. If you’re familiar with Unicode normalization (such as NFC or NFKC), \n",
    "# this is also something the tokenizer may apply.\n",
    "\n",
    "tokenizer = AutoTokenizerFast.from_pretrained(\"\")\n",
    "text_normalized = tokenizer.backend_tokenizer.normalizer.normalize_str(text) # to check how this operation is performed!\n",
    "pre_tokenization = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3dbe4cd899a8e4de65d916f28d380b752d24a40a9c5d3ac7aba403de6011ab44"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('NLP': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
